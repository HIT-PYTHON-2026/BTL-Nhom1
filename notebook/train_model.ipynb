{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14858123,"datasetId":9504314,"databundleVersionId":15719017},{"sourceType":"datasetVersion","sourceId":14847051,"datasetId":9496143,"databundleVersionId":15706871},{"sourceType":"datasetVersion","sourceId":14853042,"datasetId":9500385,"databundleVersionId":15713488},{"sourceType":"datasetVersion","sourceId":14794656,"datasetId":9459013,"databundleVersionId":15649100},{"sourceType":"datasetVersion","sourceId":14963608,"datasetId":9577730,"databundleVersionId":15835058},{"sourceType":"datasetVersion","sourceId":14933946,"datasetId":9556354,"databundleVersionId":15801857},{"sourceType":"datasetVersion","sourceId":14949569,"datasetId":9567913,"databundleVersionId":15819583},{"sourceType":"datasetVersion","sourceId":14958980,"datasetId":9574779,"databundleVersionId":15829990}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install numpy\n!pip install torch\n!pip install matplotlib\n!pip install scikit-learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\n\nclass EmotionAwareDataset(torch.utils.data.Dataset):\n    def __init__(self, base_dataset,\n                 disgust_transform,\n                 normal_transform):\n        self.base_dataset = base_dataset\n        self.base_dataset.transform = None \n        \n        self.disgust_transform = disgust_transform\n        self.normal_transform = normal_transform\n        \n        self.disgust_idx = self.base_dataset.class_to_idx['disgust']\n        \n    def __len__(self):\n        return len(self.base_dataset)\n\n    def __getitem__(self, idx):\n        image, label = self.base_dataset[idx]\n        \n        if label == self.disgust_idx:\n            image = self.disgust_transform(image)\n        else:\n            image = self.normal_transform(image)\n        return image, label\nclass DataModel:\n    def __init__(self, data_dir, batch_size, img_size=96):\n        self.data_dir = data_dir \n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.mean = [0.485, 0.456, 0.406]\n        self.std = [0.229, 0.224, 0.225]\n\n    def get_transforms(self):\n        disgust_transform = transforms.Compose([\n            transforms.RandomResizedCrop(self.img_size, scale=(0.8, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(15),\n            transforms.ColorJitter(0.2, 0.2, 0.1),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std)\n        ])\n        normal_transform = transforms.Compose([\n            transforms.RandomResizedCrop(self.img_size, scale=(0.9, 1.0)),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomApply([\n            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)\n            ], p=0.8),\n            transforms.RandomGrayscale(p=0.2), # Giúp model không quá phụ thuộc vào màu sắc\n            transforms.RandomRotation(20),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std),\n            transforms.RandomErasing(p=0.25, scale=(0.02, 0.15), ratio=(0.3, 3.3))\n        ])\n        test_transform = transforms.Compose([\n            transforms.Resize((self.img_size, self.img_size)),\n            transforms.ToTensor(),\n            transforms.Normalize(self.mean, self.std)\n        ])\n\n        return {\n            'disgust': disgust_transform,\n            'normal': normal_transform,\n            'test': test_transform\n        }\n\n    def create_data(self): \n        ts = self.get_transforms()\n        \n        train_path = os.path.join(self.data_dir, 'train')\n        test_path = os.path.join(self.data_dir, 'test')\n\n        base_train = datasets.ImageFolder(root=train_path)\n        train_set = EmotionAwareDataset(\n            base_train,\n            disgust_transform=ts['disgust'], \n            normal_transform=ts['normal']\n        )\n        \n        test_set = datasets.ImageFolder(root=test_path, transform=ts['test'])\n        \n        train_loader = DataLoader(train_set, batch_size=self.batch_size,\n                                  shuffle = True,\n                                  num_workers=4, pin_memory=True)\n        test_loader = DataLoader(test_set, batch_size=self.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n\n        return train_loader, test_loader, len(test_set.classes), test_set.classes\ndm = DataModel(data_dir='/kaggle/input/datasets/anmeo6/datasett/dataset', batch_size=32)\ntrain_loader, test_loader, num_classes, names = dm.create_data()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\ndef conv3x3(in_channels,out_channels,stride=1):\n    return nn.Conv2d(in_channels,out_channels,kernel_size= 3,stride=stride,padding= 1, bias= False)\nclass Block(nn.Module):\n    expansion = 1\n    def __init__(self, inchannels,outchannels,stride=1,downsample = None):\n        super(Block,self).__init__()\n        self.conv1 = conv3x3(inchannels,outchannels,stride)\n        self.bn1 = nn.BatchNorm2d(outchannels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(outchannels,outchannels)\n        self.bn2 = nn.BatchNorm2d(outchannels)\n         \n        self.downsample = downsample\n        self.stride =stride\n\n    def forward(self,x):\n        identity = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n        out += identity\n        out = self.relu(out)\n        return out\nclass Resnet(nn.Module):\n    def __init__(self,block,layer,num_classes):\n        super(Resnet,self).__init__()\n        self.inchannels = 64\n        self.conv1 = nn.Conv2d(3,64,kernel_size=3,stride=1,padding=(1,1),bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.layer1 = self.layer(block, 64,layer[0])\n        self.layer2 = self.layer(block, 128,layer[1],stride = 2)\n        self.layer3 = self.layer(block, 256,layer[2],stride = 2)\n        self.layer4 = self.layer(block, 512,layer[3],stride = 2)\n        \n        self.avgpooling = nn.AdaptiveAvgPool2d((1,1))\n        self.fc = nn.Sequential(\n            nn.Linear(512 * block.expansion, 512),\n            nn.BatchNorm1d(512),\n            nn.GELU(),\n            nn.Dropout(p=0.4), \n            nn.Linear(512, num_classes)\n        )\n    def layer(self,block,channels,blocks,stride = 1):\n        downsample = None\n        if stride != 1 or self.inchannels != channels*block.expansion:\n            downsample = nn.Sequential(nn.Conv2d(self.inchannels,channels*block.expansion, kernel_size= 1,stride = stride, bias = False)\n                                       ,nn.BatchNorm2d(channels*block.expansion))\n        layers = []\n        layers.append(block(self.inchannels,channels,stride,downsample))\n        self.inchannels = channels*block.expansion\n        for i in range(1,blocks):\n            layers.append(block(self.inchannels,channels))\n        return nn.Sequential(*layers)\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpooling(x)\n        x = torch.flatten(x,1)\n        x = self.fc(x)\n        return x\ndef Resnet18(num_classes):\n        return Resnet(Block,[2,2,2,2],num_classes)\nmodel = Resnet18(num_classes=num_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn.functional as F\n\ndevice =torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing = 0.1)\noptimizer = optim.AdamW([\n    {'params': model.conv1.parameters(), 'lr': 1e-5},\n    {'params': model.layer1.parameters(), 'lr': 1e-5},\n    {'params': model.layer2.parameters(), 'lr': 5e-5},\n    {'params': model.layer3.parameters(), 'lr': 1e-4},\n    {'params': model.layer4.parameters(), 'lr': 1e-4},\n    {'params': model.fc.parameters(), 'lr': 5e-4},\n], weight_decay=5e-2)\nepochs = 50\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\nmax1 = 0\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0.0\n    train_acc = 0.0\n    for i, (inputs,labels) in enumerate(train_loader):\n        inputs,labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        preds = torch.argmax(outputs, 1)\n        loss = criterion(outputs,labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * inputs.size(0)\n        train_acc += torch.sum(preds == labels.data)\n    epoch_train_loss = train_loss / len(train_loader.dataset)\n    epoch_train_acc = train_acc.double() / len(train_loader.dataset)\n    scheduler.step()\n    model.eval()\n    test_loss = 0.0\n    test_acc = 0.0\n    with torch.no_grad(): \n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            preds = torch.argmax(outputs, 1)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item() * inputs.size(0)\n            test_acc += torch.sum(preds == labels.data)\n    epoch_test_loss = test_loss / len(test_loader.dataset)\n    epoch_test_acc = test_acc.double() / len(test_loader.dataset)\n    if epoch_test_acc > max1:\n        max1 = epoch_test_acc\n        torch.save(model, 'phanloaicamxuc_7.pth')\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    print(f\"Train - Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f}\")\n    print(f\"test   - Loss: {epoch_test_loss:.4f} Acc: {epoch_test_acc:.4f}\")\nmodel = torch.load('phanloaicamxuc_7.pth',weights_only=False) \nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report # Thêm classification_report\n\ndef plot_confusion_matrix(model, test_loader, device, classes):\n    model.eval()\n    y_true = []\n    y_pred = []\n    \n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            preds = torch.argmax(outputs, 1)\n            \n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n    \n    # 1. Tính toán và In báo cáo chi tiết (Precision, Recall, F1)\n    print(\"\\n\" + \"=\"*20 + \" CHI TIẾT ĐÁNH GIÁ (CLASSIFICATION REPORT) \" + \"=\"*20)\n    report = classification_report(y_true, y_pred, target_names=classes)\n    print(report)\n    print(\"=\"*80)\n\n    # 2. Tính ma trận nhầm lẫn\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # 3. Vẽ biểu đồ Heatmap\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=classes, yticklabels=classes)\n    plt.xlabel('Dự đoán (Predicted)')\n    plt.ylabel('Thực tế (Actual)')\n    plt.title('Confusion Matrix - Phân loại cảm xúc')\n    plt.show()\n\n# Gọi hàm sau khi train xong\nclasses = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nplot_confusion_matrix(model, test_loader, device, classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}